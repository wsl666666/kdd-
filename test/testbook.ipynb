{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sys import getsizeof\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node = 0\n",
    "node_labels = np.ones(max_node, dtype=int) * max_node\n",
    "print( 'node-label',  round(getsizeof(node_labels) / 1024 / 1024 /1024 ,2) )\n",
    "\n",
    "node_feature = np.ones((max_node, 300), dtype=float)\n",
    "print( 'node-feat',  round(getsizeof(node_feature) / 1024 / 1024/ 1024,2) )\n",
    "\n",
    "# edge_feature = np.ones((max_node*max_node*0.003, 300), dtype=int)\n",
    "# print( 'node-feat',  round(getsizeof(node_labels) / 1024 / 1024,2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(getsizeof(node_labels) / 1024 / 1024,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse vector\n",
    "round(getsizeof(node_labels) / 1024 / 1024 ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty(shape=(0),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3], dtype=int)\n",
    "np.hstack((a, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_events_dict: Dict[Tuple[int, int], int] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # E = [\n",
    "    #     (0,1,1,1,1),\n",
    "    #     (0,2,1,1,1),\n",
    "    #     (0,3,1,1,1),\n",
    "    #     (0,4,1,1,1),\n",
    "    #     ]\n",
    "    # # new snapshot...\n",
    "    # E_new = [\n",
    "    #     [(0,2,1,1,1),(2,3,1,1,1)],\n",
    "    # ]\n",
    "    # snapshot_ppv = test_dynamic_ppr_batch_source(E, E_new)\n",
    "    # print (snapshot_ppv)\n",
    "    # print(f'Threading layer chosen: {numba.threading_layer()}' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa(a:int, b:int, *args, **kwargs):\n",
    "    # print('1', a,b)\n",
    "    # print('2', args)\n",
    "    # print('3', kwargs)\n",
    "    pass\n",
    "    \n",
    "def bb(a:int, b:int, kwargs):\n",
    "    print(kwargs)\n",
    "    kwargs['a+b'] = a+b+2\n",
    "    print('3', kwargs)\n",
    "    return None\n",
    "    \n",
    "\n",
    "tmp_kwargs = {'1':99}\n",
    "\n",
    "# aa(1,2, tmp_kwargs)\n",
    "_ = bb(1,2, tmp_kwargs)\n",
    "\n",
    "print(tmp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare numba speed with nnz\n",
    "### conclusion: input dense vector, output dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np \n",
    "import time\n",
    "from numba.typed import Dict as nb_dict\n",
    "from numba.core import types\n",
    "import os\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/tmp\"\n",
    "\n",
    "\n",
    "def test_input_dense_vector_python(p_s: np.ndarray, r_s:np.ndarray, alpha: float, rand_order: np.ndarray):\n",
    "    total = rand_order.shape[0]\n",
    "    for _ in range(total):\n",
    "        i = rand_order[_]\n",
    "        p_s[i] = p_s[i] + p_s[i//2] + alpha* r_s[i]\n",
    "    return p_s, p_s, r_s, r_s\n",
    "\n",
    "@nb.njit(fastmath = True, parallel =True, nogil = True)\n",
    "def test_input_dense_vector_numba(p_s: np.ndarray, r_s:np.ndarray, alpha: float, rand_order: np.ndarray):\n",
    "    total = rand_order.shape[0]\n",
    "    for _ in nb.prange(total):\n",
    "        i = rand_order[_]\n",
    "        p_s[i] = p_s[i] + p_s[i//2] + alpha* r_s[i]\n",
    "    return p_s, p_s, r_s, r_s\n",
    "\n",
    "@nb.njit(fastmath = True, parallel =True, nogil = True)\n",
    "def test_input_dense_vector_numba_inplace(p_s: np.ndarray, r_s:np.ndarray, alpha: float, rand_order: np.ndarray):\n",
    "    total = rand_order.shape[0]\n",
    "    for _ in nb.prange(total):\n",
    "        i = rand_order[_]\n",
    "        p_s[i] = p_s[i] + p_s[i//2] + alpha* r_s[i]\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "@nb.njit(fastmath = True, parallel =True, nogil = True)\n",
    "def test_input_dense_vector_numba_return_nnz_idx(p_s: np.ndarray, r_s:np.ndarray, alpha: float, rand_order: np.ndarray):\n",
    "    total = rand_order.shape[0]\n",
    "    for _ in nb.prange(total):\n",
    "        i = rand_order[_]\n",
    "        p_s[i] = p_s[i] + p_s[i//2] + alpha* r_s[i]\n",
    "    # return sparse\n",
    "    p_s_nnz_idx = np.nonzero(p_s)\n",
    "    # p_s_nnz_val = p_s[p_s_nnz_idx]\n",
    "    r_s_nnz_idx = np.nonzero(r_s)\n",
    "    # r_s_nnz_val = r_s[r_s_nnz_idx]\n",
    "    return p_s_nnz_idx, p_s_nnz_idx, r_s_nnz_idx, r_s_nnz_idx\n",
    "\n",
    "\n",
    "@nb.njit(fastmath = True, parallel =True, nogil = True)\n",
    "def test_input_dense_vector_numba_return_nnz_idx_val(p_s: np.ndarray, r_s:np.ndarray, alpha: float, rand_order: np.ndarray):\n",
    "    total = rand_order.shape[0]\n",
    "    for _ in nb.prange(total):\n",
    "        i = rand_order[_]\n",
    "        p_s[i] = p_s[i] + p_s[i//2] + alpha* r_s[i]\n",
    "    # return sparse\n",
    "    p_s_nnz_idx = np.nonzero(p_s)\n",
    "    p_s_nnz_val = p_s[p_s_nnz_idx]\n",
    "    r_s_nnz_idx = np.nonzero(r_s)\n",
    "    r_s_nnz_val = r_s[r_s_nnz_idx]\n",
    "    return p_s_nnz_idx, p_s_nnz_val, r_s_nnz_idx, r_s_nnz_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_speed(func):\n",
    "    n = 1000000\n",
    "    num_ops = 5000\n",
    "    alpha = 0.2\n",
    "    s = 3\n",
    "    rand_order = np.random.choice(n, num_ops)\n",
    "    repeat_time = 1000\n",
    "    comsumed_time = []\n",
    "    for _ in range(repeat_time):\n",
    "        p_s = np.zeros(n, dtype=float)\n",
    "        r_s = np.ones(n, dtype=float)\n",
    "        # print(f'sum of ps {np.sum(p_s)}')\n",
    "        start = time.perf_counter()\n",
    "        p_s_nnz_idx, p_s_nnz_val, r_s_nnz_idx, r_s_nnz_val = func(p_s, r_s, alpha, rand_order)\n",
    "        comsumed_time.append( time.perf_counter() - start)\n",
    "        # print(f'sum of ps {np.sum(p_s)}')\n",
    "    print(f'Testing: {func.__name__}: n: {n}, num_ops:{num_ops}')\n",
    "    print(f'Speed per iter: {np.mean(comsumed_time)*1000} ms per with std {np.std(comsumed_time *1000)}')\n",
    "\n",
    "test_speed(test_input_dense_vector_python)\n",
    "test_speed(test_input_dense_vector_numba)\n",
    "test_speed(test_input_dense_vector_numba_inplace)\n",
    "test_speed(test_input_dense_vector_numba_return_nnz_idx)\n",
    "test_speed(test_input_dense_vector_numba_return_nnz_idx_val)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Testing: test_input_dense_vector_python: n: 1000000, num_ops:5000\n",
    "Speed per iter: 4.179927775170654 ms per with std 0.0002740929690304788\n",
    "Testing: test_input_dense_vector_numba: n: 1000000, num_ops:5000\n",
    "Speed per iter: 0.1503849932923913 ms per with std 1.1700934376113088e-05\n",
    "Testing: test_input_dense_vector_numba_inplace: n: 1000000, num_ops:5000\n",
    "Speed per iter: 0.13864837167784572 ms per with std 2.006728684142806e-05\n",
    "Testing: test_input_dense_vector_numba_return_nnz_idx: n: 1000000, num_ops:5000\n",
    "Speed per iter: 2.5898508434183896 ms per with std 8.440914670536068e-05\n",
    "Testing: test_input_dense_vector_numba_return_nnz_idx_val: n: 1000000, num_ops:5000\n",
    "Speed per iter: 3.821231385692954 ms per with std 0.0002650383047438655\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 2000\n",
    "dict_p_arr = nb_dict.empty(key_type = types.int32, value_type = types.float32[:])\n",
    "tracked_nodes = np.arange(1,10, dtype=np.int32)\n",
    "print(tracked_nodes)\n",
    "\n",
    "@nb.njit(nogil = True)\n",
    "def test_numba_dict_ndarray(dict_p_arr, tracked_nodes, n):\n",
    "    _ = tracked_nodes.shape[0]\n",
    "    for i in nb.prange(_):\n",
    "        dict_p_arr[i] = np.ones(n, dtype = np.float32)\n",
    "    print(dict_p_arr, tracked_nodes)\n",
    "    \n",
    "print(test_numba_dict_ndarray(dict_p_arr, tracked_nodes, n))\n",
    "print(dict_p_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @nb.njit(nogil = True, parallel = True)\n",
    "@nb.njit(nogil = True, parallel = True, fastmath = True)\n",
    "def change_val_multithread(num_ops, total_n, dim, nb_dict):\n",
    "    for i in nb.prange(total_n):\n",
    "        for j in nb.prange(dim):\n",
    "            for _ in range(num_ops): \n",
    "                nb_dict[i][j] = nb_dict[i][j] + nb_dict[i][j//2]\n",
    "\n",
    "def test_multithread():\n",
    "    n = 200\n",
    "    dim = 3000\n",
    "    num_ops = 100\n",
    "    dict_p_arr = nb_dict.empty(key_type = types.int32, value_type = types.float32[:])\n",
    "    for _ in np.arange(n, dtype=np.int32):\n",
    "        dict_p_arr[_] = np.ones( dim,dtype=np.float32)\n",
    "    change_val_multithread(num_ops, n, dim, dict_p_arr)\n",
    "    return dict_p_arr\n",
    "\n",
    "dict_p_arr = test_multithread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nb.njit(nogil = True, parallel = True, fastmath = True)\n",
    "def set_val_multithread(num_ops, total_n, dim, nb_dict):\n",
    "    for i in nb.prange(total_n):\n",
    "        visit_set = set()\n",
    "        for j in range(dim):\n",
    "            for _ in range(num_ops): \n",
    "                if j not in visit_set:\n",
    "                    visit_set.add(j)\n",
    "                    nb_dict[i][j] = nb_dict[i][j]*1.87 + nb_dict[i][j//2]\n",
    "                    visit_set.add(nb_dict[i][j])\n",
    "\n",
    "def test_set_val_multithread():\n",
    "    n = 200\n",
    "    dim = 3000\n",
    "    num_ops = 100\n",
    "    dict_p_arr = nb_dict.empty(key_type = types.int32, value_type = types.float32[:])\n",
    "    for _ in np.arange(n, dtype=np.int32):\n",
    "        dict_p_arr[_] = np.ones( dim,dtype=np.float32)\n",
    "    set_val_multithread(num_ops, n, dim, dict_p_arr)\n",
    "    return dict_p_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_p_arr = test_set_val_multithread()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(np.ones(1000*6000000, dtype=np.float32))/1024/1024\n",
    "# propagation with sparsity trade off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(1.0 / 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "S = csr_matrix([[0, 1], [0, 5]]).sum(axis=1)\n",
    "print(S)\n",
    "S[S!=0] = S[S != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "S = np.array([2.0,0,2.0])\n",
    "S[S != 0] = 1.0 / S[S != 0]\n",
    "print(S)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([2,0,2], dtype = np.float32)\n",
    "S[S != 0] = np.array([0.5, 0.5])\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut =20\n",
    "a = np.array([230, 10, 284, 39, 76])\n",
    "a[a != cut] = 1.0/(a[a != cut])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "edge_event = []\n",
    "for t, (u, v) in enumerate(G.edges):\n",
    "    edge_event.append((u, v, 1.0, t))\n",
    "    edge_event.append((v, u, 1.0, t))\n",
    "\n",
    "f_path = '/home/xingzguo/projects_data/DynMixer/karate/raw_data/edge_list.json'\n",
    "with open(f_path, 'w') as f:\n",
    "    f.write('\\n'.join([json.dumps(_) for _ in edge_event]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestA:\n",
    "    def __init__(self) -> None:\n",
    "        self.num = 0\n",
    "    \n",
    "    def update_cb(self, num):\n",
    "        print(self.num, num )\n",
    "        self.num += num\n",
    "        \n",
    "\n",
    "def external_add(num_in, cb_func=None):\n",
    "    if cb_func:\n",
    "        cb_func(num_in)\n",
    "        print(cb_func)\n",
    "\n",
    "\n",
    "tst = TestA()\n",
    "\n",
    "numbs = [1,2,3,4]\n",
    "for num in numbs:\n",
    "    external_add(num, cb_func = tst.update_cb)\n",
    "\n",
    "print(tst.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np \n",
    "import time\n",
    "from numba.typed import Dict as nb_dict\n",
    "from numba.core import types\n",
    "import os\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/tmp\"\n",
    "import networkx as nx\n",
    "\n",
    "def conjugate_gradient(\n",
    "    N: int,\n",
    "    query_list: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    indptr: np.ndarray,\n",
    "    data: np.ndarray,\n",
    "    node_degree: np.ndarray,\n",
    "    dict_p_arr: nb_dict,\n",
    "    dict_r_arr: nb_dict,\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    "    init_epsilon: float,\n",
    "):\n",
    "    # check number of nodes.\n",
    "    # the out-degree of nodes\n",
    "    eps_prime = np.float32(init_epsilon / node_degree.sum())\n",
    "    for i in nb.prange(query_list.shape[0]):\n",
    "        # Multi-thread if using numba's nonpython mode. No GIL\n",
    "        # Adaptive push according to p_s\n",
    "        s = query_list[i]\n",
    "        adapt_epsilon = np.float32(\n",
    "            eps_prime * node_degree[s]\n",
    "        )  # for high degree nodes?\n",
    "        adapt_epsilon = np.max(\n",
    "            np.array([adapt_epsilon, 1e-3])\n",
    "        )  # the lower bound\n",
    "        epsilon = np.min(\n",
    "            np.array([adapt_epsilon, 1e-3])\n",
    "        )  # the upper bound\n",
    "        \n",
    "        \n",
    "        #p_s is the init vector\n",
    "        m = data.shape[0]\n",
    "        lap_mat: np.ndarray = np.zeros(m, dtype = np.float32)\n",
    "        p_s: np.ndarray = dict_p_arr[s]\n",
    "        r:np.ndarray = np.zeros(N, dtype = np.float32)\n",
    "        p:np.ndarray = np.zeros(N, dtype = np.float32)\n",
    "        gpr_vec = p_s\n",
    "        mat_x_vec = np.zeros(N, dtype = np.int32)\n",
    "        p_vec = np.zeros(N, dtype = np.float32)\n",
    "        p_vec[s] = 1.0\n",
    "        for uu in range(N):\n",
    "            for index in range(indptr[uu], indptr[uu + 1]):\n",
    "                lap_mat[index] = alpha / (np.sqrt(node_degree[uu]) * np.sqrt(node_degree[indices[index]]))\n",
    "                mat_x_vec[uu] += lap_mat[index] * gpr_vec[indices[index]]\n",
    "        rs_old:np.float32 = 0.0\n",
    "        for uu in range(N):\n",
    "            r[uu] = (1. - alpha) * p_vec[uu] / np.sqrt(node_degree[uu] - gpr_vec[uu] + mat_x_vec[uu])\n",
    "            p[uu] = r[uu]\n",
    "            rs_old += r[uu] * r[uu]\n",
    "            \n",
    "        max_iter = 1000\n",
    "        for epoch in range(max_iter):\n",
    "            mat_x_vec = p.copy()\n",
    "            for uu in range(N):\n",
    "                alpha_sum = 0.0\n",
    "                for index in range(indptr[uu], indptr[uu + 1]):\n",
    "                    alpha_sum += lap_mat[index] * p[indices[index]]\n",
    "                mat_x_vec[uu] -= alpha_sum;\n",
    "            alpha_sum = 0.0\n",
    "            for uu in range(N):\n",
    "                alpha_sum += mat_x_vec[uu] * p[uu]\n",
    "            alpha_sum = rs_old / alpha_sum\n",
    "            rs_new = 0.0\n",
    "            for uu in range(N):\n",
    "                gpr_vec[uu] += alpha_sum * p[uu];\n",
    "                r[uu] -= alpha_sum * mat_x_vec[uu];\n",
    "                rs_new += r[uu] * r[uu];\n",
    "            \n",
    "            if np.sqrt(rs_new) < epsilon:\n",
    "                break\n",
    "            for uu in range(N):\n",
    "                p[uu] = r[uu] + (rs_new / rs_old) * p[uu]\n",
    "            rs_old = rs_new\n",
    "        \n",
    "        for uu in range(N):\n",
    "            gpr_vec[uu] *= np.sqrt(node_degree[uu])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np \n",
    "import time\n",
    "from numba.typed import Dict as nb_dict\n",
    "from numba.core import types\n",
    "import os\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/tmp\"\n",
    "import networkx as nx\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def ppr_ista(\n",
    "    N: int,\n",
    "    query_list: np.ndarray,\n",
    "    indices: np.ndarray,\n",
    "    indptr: np.ndarray,\n",
    "    data: np.ndarray,\n",
    "    node_degree: np.ndarray,\n",
    "    alpha_norm: float,\n",
    "    rho:float,\n",
    "    max_iter:int = 1000\n",
    "):\n",
    "    # lazy walk is equivalent to random walk with adjusted alpha.\n",
    "    alpha = alpha_norm/(2.0-alpha_norm) \n",
    "    A = sp.csr_matrix((data, indices, indptr), shape = (N,N))\n",
    "    d_mat = sp.diags(node_degree, dtype = np.float32)\n",
    "    sqrt_d_out = np.sqrt(node_degree)\n",
    "    nsqrt_d_out = 1.0/sqrt_d_out\n",
    "    nsqrt_d_out_mat = sp.diags(nsqrt_d_out, dtype = np.float32)\n",
    "    # Q = 0.5*(1+alpha)*(sp.identity(N, dtype = np.float32) - \\\n",
    "    #     nsqrt_d_out_mat@ A @nsqr8t_d_out_mat)\n",
    "    Q = nsqrt_d_out_mat @(d_mat - 0.5*(1-alpha) *(d_mat + A)) @ nsqrt_d_out_mat\n",
    "    cond_vec = rho*alpha* (sqrt_d_out)\n",
    "    \n",
    "    for s_id in query_list:\n",
    "        q = np.zeros(N, dtype=np.float32)\n",
    "        grad_f = np.zeros(N, dtype=np.float32)\n",
    "        grad_f[s_id] = -alpha* (nsqrt_d_out[s_id])\n",
    "        \n",
    "        Cc = np.zeros(N, dtype = np.float32)\n",
    "        Cc[s_id] = alpha*nsqrt_d_out[s_id]\n",
    "\n",
    "        for k in range(max_iter):\n",
    "            for i in range(N):\n",
    "                if q[i] - grad_f[i] >= cond_vec[i]:\n",
    "                    q[i] = q[i] - (grad_f[i] + cond_vec[i])\n",
    "                elif q[i] - grad_f[i] <= -cond_vec[i]:\n",
    "                    q[i] = q[i] - (grad_f[i] - cond_vec[i])\n",
    "                else:\n",
    "                    q[i] = 0.0\n",
    "            # get gradient. Qq - alpha*nsqrt_d*e_i\n",
    "            \n",
    "            # for u in range(N):\n",
    "            #     Qv = Q.indices[Q.indptr[u] : Q.indptr[u + 1]]\n",
    "            #     Qw = Q.data[Q.indptr[u] : Q.indptr[u + 1]]\n",
    "            #     for i in range(Qv.shape[0]):\n",
    "            #         _v = Qv[i]\n",
    "            #         _Q_u_v = Qw[i]\n",
    "            #         Qq[u] += _Q_u_v * q[_v]\n",
    "            grad_f = Q@q - Cc\n",
    "            \n",
    "        ppv = np.zeros(N, dtype = np.float32) \n",
    "        for i in range(N):\n",
    "            ppv[i] = sqrt_d_out[i]*q[i]\n",
    "        print(ppv)\n",
    "            \n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "csr_graph = nx.to_scipy_sparse_matrix(G.to_undirected())\n",
    "node_degree = np.squeeze(np.array(csr_graph.sum(axis = 1))).astype(np.float32)\n",
    "ppr_ista(\n",
    "    csr_graph.shape[0],\n",
    "    np.array([0,1,2]),\n",
    "    csr_graph.indices,\n",
    "    csr_graph.indptr,\n",
    "    csr_graph.data,\n",
    "    node_degree,\n",
    "    alpha_norm=0.2, \n",
    "    rho = 1e-5,\n",
    "    max_iter = 100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "csr_graph = nx.to_scipy_sparse_matrix(G.to_undirected())\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank_scipy\n",
    "\n",
    "for s_id in np.array([0,1,2]):\n",
    "    p_dict = pagerank_scipy(\n",
    "        G.to_undirected(),\n",
    "        alpha=0.8, #damping\n",
    "        personalization={s_id:1},\n",
    "        max_iter=1000,\n",
    "        tol=1.0e-6,\n",
    "        nstart=None,\n",
    "        weight=None,\n",
    "        dangling=None,\n",
    "    )\n",
    "\n",
    "    ppr_val = np.array([ p_dict[_] for _ in list(G)])\n",
    "    print(s_id, ppr_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as os_join\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "project_data_folder = '/home/xingzguo/projects_data/DynMixer'\n",
    "dataset_name = 'citeseer'\n",
    "dataset_path = os_join(project_data_folder, dataset_name)\n",
    "if os.path.exists(dataset_path) is not True:\n",
    "    os.makedirs(dataset_path)\n",
    "\n",
    "dataset = torch_geometric.datasets.Planetoid(\n",
    "    dataset_path, name=dataset_name, transform=T.NormalizeFeatures()\n",
    ")\n",
    "\n",
    "# E = dataset[0].edge_index.numpy().astype(np.int32)\n",
    "# N = dataset[0].x.shape[0]\n",
    "# csr_mat = csr_matrix(\n",
    "#     (np.ones_like(E[0]).astype(np.int32), (E[0], E[1])), shape=(N, N)\n",
    "# )\n",
    "\n",
    "# give out raw features\n",
    "# give out...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = dataset[0].edge_index.numpy().astype(np.int32)\n",
    "X = dataset[0].x.numpy()\n",
    "print(np.unique(dataset[0].y))\n",
    "print(np.unique(E.flatten()).shape)\n",
    "print(X.shape)\n",
    "print(E.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "nb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "structure_events_dict: Dict[\n",
    "    Tuple[int, int],\n",
    "    Tuple[int, int]\n",
    "] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [\"a\", \"b\", \"c\"] \n",
    "for i, v in enumerate(a_list):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "a = np.array([[0.1,0.2,0.3], [0.9,0.12,0.343]])\n",
    "b = np.array([[0.12,0.235,0.332], [0.952,0.1322,0.34243]])\n",
    "\n",
    "a2 = np.array([[0.1,0.2212,0.253], [0.87,0.1432,0.3433]])\n",
    "b2 = np.array([[0.12,0.24335,0.332], [0.952,0.13222,0.34243]])\n",
    "\n",
    "l1_list  = np.array([np.sum(a-b, axis = 1), np.sum(a2-b2, axis = 1)])\n",
    "l1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_list.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils import murmurhash3_32 as murmurhash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "a =+ 3\n",
    "a\n",
    "# feat_mat = torch.randint(0,100,(5,100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32 as murmurhash\n",
    "import numba \n",
    "def get_hash_LUT(n:int,dim:int = 512,rnd_seed:int = 0 ):\n",
    "    node_id_2_dim_id = np.zeros(n,dtype=np.int32)\n",
    "    node_id_2_sign = np.zeros(n,dtype=np.int8)\n",
    "    for _ in range(n):\n",
    "        dim_id = murmurhash(_, seed = rnd_seed,positive=True)%dim\n",
    "        sign = murmurhash(_, seed = rnd_seed,positive=True)%2\n",
    "        node_id_2_dim_id[_] = dim_id\n",
    "        node_id_2_sign[_] = 1 if sign == 1 else -1\n",
    "    return node_id_2_dim_id, node_id_2_sign\n",
    "\n",
    "@numba.njit(cache =False, parallel = True, fastmath=True, nogil = True)\n",
    "def get_hash_embed(node_id_2_dim_id, node_id_2_sign, dim, n, indices, indptr, data):\n",
    "    emb_mat = np.zeros((n,dim), dtype=np.float32)\n",
    "    for i in numba.prange(n): # for all nodes.\n",
    "        cols = indices[indptr[i]:indptr[i + 1]]\n",
    "        vals = data[indptr[i]:indptr[i + 1]]\n",
    "        emb_vec = emb_mat[i,:]\n",
    "        for j, val in zip(cols,vals):\n",
    "            emb_vec[node_id_2_dim_id[j]] += node_id_2_sign[j]* np.max( np.array([np.float32(0.0), np.float32(np.log(val*n))]) )\n",
    "    return emb_mat\n",
    "\n",
    "node_id_2_dim_id, node_id_2_sign = get_hash_LUT(n=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_2_dim_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_2_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random((4,4)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "crow_indices = torch.tensor([0, 2, 4])\n",
    "col_indices = torch.tensor([0, 1, 0, 1])\n",
    "values = torch.tensor([1, 2, 3, 4])\n",
    "csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n",
    "csr.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices= np.array([0, 1, 0, 1]) # \n",
    "indptr = np.array([0, 2, 4]) # idx is row.\n",
    "data = np.array([1, 2, 3, 4])\n",
    "\n",
    "csr_mat = sp.csr_matrix((data, indices, indptr), shape = (2,2))\n",
    "csr_mat.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random((4,5))\n",
    "row_sum = x.sum(axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x/row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_max, scatter_add, segment_csr\n",
    "\n",
    "src = torch.tensor([[1,1,1,1], [0.5,2,2,2], [2,3,3,3]])\n",
    "indptr = torch.tensor([0, 2, 2])\n",
    "# indptr = indptr.view(1, -1)  \n",
    "\n",
    "segment_csr(src, indptr, reduce=\"mean\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed benchmark\n",
    "- scipy-sparse vs torch-sparse\n",
    "- nnz of ppv is kept same regardless of graph size.\n",
    "- **conclusion:**: torch.sparse.mm is 3-4x faster than scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_speed_benchmark = pd.read_json('../src/Mixer/benchmark_stats/df_speed_benchmark.json')\n",
    "# df_speed_benchmark[df_speed_benchmark['use_torch_sparse'] == True]\n",
    "table = pd.pivot_table( df_speed_benchmark, values = ['time_mean', 'time_std','nnz_mean'], index=['total_num_nodes', 'aggregate_type', 'use_torch_sparse',], )\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "nnz_lb_node_ids = np.array([0,1,2,3,5])\n",
    "nnz_lb = np.array([1,2,3,4,5])\n",
    "pairs = np.vstack((nnz_lb_node_ids, nnz_lb))\n",
    "outd = np.array([0,1,2,3,4,0,1,2])\n",
    "nnz_lb_node_outd = outd[nnz_lb_node_ids]\n",
    "non_dangling_mask = nnz_lb_node_outd>0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_lb_node_outd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dangling_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[:,non_dangling_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import scipy.sparse as sp\n",
    "\n",
    "crow_indices = torch.tensor([0, 2, 4])\n",
    "col_indices = torch.tensor([0, 1, 0, 1])\n",
    "values = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n",
    "print(csr.to_dense())\n",
    "torch.topk(csr.to_dense(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# input\n",
    "a=np.array([[1,2,3,4],[8,7,6,5],[5,3,1,2]])\n",
    "top_k = 8\n",
    "if top_k>a.shape[1]:\n",
    "    top_k = a.shape[1]\n",
    "    \n",
    "mask = np.argpartition(a, -top_k, axis=1) < a.shape[1] - top_k\n",
    "a[mask] = 0\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([1,2,3,0,0,0])\n",
    "assert S[S == 0.0].shape[0] == 0, \"graph has dangling nodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "def topk_norm(max_top_k: int, matrix_input: np.ndarray, ord_norm: int = 1):\n",
    "    \"\"\"take top-k element per row and re-normalize row-wise as per the\n",
    "    ord (e.g., l-1 norm)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    out_mat = np.zeros_like(matrix_input)\n",
    "\n",
    "    m = out_mat.shape[1]\n",
    "    if max_top_k > m:\n",
    "        max_top_k = m\n",
    "    sort_k = m - max_top_k\n",
    "    # mask lowest vals per row\n",
    "    large_idx = np.argpartition(matrix_input, sort_k, axis=1)[:, sort_k:]\n",
    "    row_idx = np.arange(matrix_input.shape[0]).reshape(-1, 1)\n",
    "    out_mat[row_idx, large_idx] = matrix_input[row_idx, large_idx]\n",
    "    l1_norm = np.linalg.norm(out_mat, axis=1, keepdims=True, ord=ord_norm)\n",
    "\n",
    "    return (out_mat / l1_norm).astype(np.float32)\n",
    "\n",
    "\n",
    "# matrix= np.array([[1.7747078e-06, 2.8687605e-04, 5.9040769e-05, 0.00, ], [1.77078e-06 ,2.887605e-04 ,5.90769e-05, 0.00]])\n",
    "matrix= np.array([[1.7747078e-06, 0.0, 5.9040769e-05, 0.00, ], [0.0 ,2.887605e-04 ,5.90769e-05, 0.00]])\n",
    "\n",
    "print(matrix)\n",
    "topk_norm(3, matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottomk_norm(btm_k: int, matrix_input: np.ndarray, ord_norm: int = 1):\n",
    "    \"\"\"take top-k element per row and re-normalize row-wise as per the\n",
    "    ord (e.g., l-1 norm)\n",
    "    ignore zero entries\n",
    "\n",
    "    \"\"\"\n",
    "    out_mat = np.zeros_like(matrix_input)\n",
    "    matrix_input = np.array(matrix_input, copy=True)\n",
    "    matrix_input[matrix_input == 0.0] = np.Inf\n",
    "\n",
    "    m = out_mat.shape[1]\n",
    "    if btm_k > m:\n",
    "        btm_k = m\n",
    "        \n",
    "    sort_k = btm_k# m - btm_k\n",
    "    print(sort_k)\n",
    "    # mask lowest vals per row\n",
    "    small_idx = np.argpartition(matrix_input, sort_k, axis=1)[:, :sort_k]\n",
    "    print(small_idx)\n",
    "    row_idx = np.arange(matrix_input.shape[0]).reshape(-1, 1)\n",
    "    out_mat[row_idx, small_idx] = matrix_input[row_idx, small_idx]\n",
    "    # in-case of nnz is smaller than requested btm-k\n",
    "    out_mat[out_mat == np.Inf] = 0.0 \n",
    "    l1_norm = np.linalg.norm(out_mat, axis=1, keepdims=True, ord=ord_norm)\n",
    "\n",
    "    return (out_mat / l1_norm).astype(np.float32)\n",
    "\n",
    "\n",
    "matrix= np.array([[1.7747078e-06, 0.0, 5.9040769e-05, 0.00, ], [1.77078e-06 ,0.0 ,5.90769e-05, 0.00]])\n",
    "print(matrix)\n",
    "\n",
    "bottomk_norm(3, matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "# @nb.jit\n",
    "def randomk_norm(\n",
    "    rnd_k: int,\n",
    "    matrix_input: np.ndarray,\n",
    "    ord_norm: int = 1,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"take random-k element per row and re-normalize row-wise as per the\n",
    "    ord (e.g., l-1 norm)\n",
    "    ignore zero entries\n",
    "\n",
    "    \"\"\"\n",
    "    out_mat = np.zeros_like(matrix_input)\n",
    "    matrix_input = np.array(matrix_input, copy=True)\n",
    "\n",
    "    m = out_mat.shape[1]\n",
    "    if rnd_k > m:\n",
    "        rnd_k = m\n",
    "\n",
    "    # random sample nnzs\n",
    "    random_nnz_idx = np.zeros((matrix_input.shape[0], rnd_k), dtype = int)\n",
    "    for i in range(matrix_input.shape[0]):\n",
    "        nnz_idx = np.nonzero(matrix_input[i])[0]\n",
    "        if nnz_idx.shape[0] == rnd_k:\n",
    "            for j in range(nnz_idx.shape[0]):\n",
    "                random_nnz_idx[i,j] = nnz_idx[j]\n",
    "        elif nnz_idx.shape[0] > rnd_k:\n",
    "            random_nnz_idx[i] = np.random.choice(nnz_idx, size=rnd_k, replace=False)\n",
    "        else:\n",
    "            random_nnz_idx[i] = np.random.choice(nnz_idx, size=rnd_k, replace=True)\n",
    "\n",
    "    row_idx = np.arange(matrix_input.shape[0]).reshape(-1, 1)\n",
    "    out_mat[row_idx, random_nnz_idx] = matrix_input[row_idx, random_nnz_idx]\n",
    "\n",
    "    l1_norm = np.linalg.norm(out_mat, axis=1, keepdims=True, ord=ord_norm)\n",
    "    out_norm_mat = (out_mat / l1_norm).astype(np.float32)\n",
    "    out_norm_mat = torch.tensor(out_norm_mat, dtype=torch.float32)\n",
    "    return out_norm_mat\n",
    "\n",
    "matrix= np.array([[1.7747078e-06, 0.982, 5.9040769e-05, 0.00, ], [1.77078e-06 ,0.0 ,5.90769e-05, 0.00]])\n",
    "print(matrix)\n",
    "randomk_norm(2, matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix= np.array([[1.7747078e-06, 0.0, 5.9040769e-05, 0.00, ], [1.77078e-06 ,0.0 ,5.90769e-05, 0.00]])\n",
    "\n",
    "np.nonzero(matrix[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "from os.path import join as os_join\n",
    "graph_dataset_name = 'ogbn-products'\n",
    "local_dataset_dir_abs_path = '/home/xingzguo/projects_data/DynMixer/'\n",
    "\n",
    "ogbn_root = os_join(local_dataset_dir_abs_path, 'ogbn_datasets')\n",
    "dataset = NodePropPredDataset(name = graph_dataset_name, root=ogbn_root,)\n",
    "# split_idx = dataset.get_idx_split()\n",
    "# train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "graph, label = dataset[0] # graph: library-agnostic graph object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_feat\n",
    "\n",
    "graph, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph['node_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph['num_nodes'])\n",
    "print(graph['edge_index'].min(), graph['edge_index'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph['edge_index'].astype(int).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph['node_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(label).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [(1,2,), (5,3), (4,5), (3,7)]\n",
    "sorted(a, key =lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1,2,), (5,3), (4,5), (3,7)]\n",
    "a.sort(key =lambda x: x[0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_curt_all_nodes_lbs = np.array([[1,0,0],[0,1,1],[0,0,1]])\n",
    "np.nonzero(graph_curt_all_nodes_lbs[[0,1,2],:])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "node_num_sample_per_lb = 1000\n",
    "train_per_lb = 0.98\n",
    "np.ceil(\n",
    "        node_num_sample_per_lb * train_per_lb, \n",
    "        ).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.random((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.random((2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.random((2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "args_js = 'asdf2'\n",
    "aa = hashlib.md5(args_js.encode(\"utf-8\")).hexdigest()\n",
    "aa+'.pkl'\n",
    "# os.path.join(aa,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.normal(0, 0.1, size = (3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "def test_func(**kwargs):\n",
    "    k = [        \n",
    "         \"alpha\",\n",
    "        \"data_strategy\",\n",
    "        \"graph_dataset_name\",\n",
    "        \"graph_snapshot_basetime\",\n",
    "        \"graph_snapshot_interval\",\n",
    "        \"is_dangling_avoid\",\n",
    "        \"ppr_algo\",\n",
    "        \"rs\",\n",
    "        \"dev_per_lb\",\n",
    "        \"test_per_lb\",\n",
    "        \"total_sampled_node\",\n",
    "        \"train_per_lb\",\n",
    "        \"use_incrmt_ppr\",]\n",
    "    hash_dict = {}\n",
    "    for key in k:\n",
    "        print(kwargs[key])\n",
    "        hash_dict[key] = kwargs[key]\n",
    "    args_js = json.dumps({**hash_dict}, sort_keys=True, default=str)\n",
    "    print(args_js)\n",
    "    hash_file = hashlib.md5(args_js.encode(\"utf-8\")).hexdigest()\n",
    "    print(hash_file)\n",
    "\n",
    "test_func(aggregate_type='mixrank', alpha=0.15, data_strategy='fix-all', dev_per_lb=0.1, exp_name='chameleon-debug-1', graph_dataset_name='squirrel', graph_snapshot_basetime=150000.0, graph_snapshot_interval=15000.0, hidden_size_mlps=[32, 16], is_dangling_avoid=True, is_retrain_each_snapshot=True, local_proj_cache_dir='/home/xingzguo/git_project/DynMixer/cache', local_proj_data_dir='/home/xingzguo/projects_data/DynMixer', min_epoch_train=5, model_max_train_epochs=1000, n_cpu=4, num_mlps=2, optim_lr=0.05, ppe_out_dim=128, ppr_algo='ista', pprgo_topk=32, rs=621, test_per_lb=0.2, total_sampled_node=2000.0, train_per_lb=0.7, use_cuda=True, use_incrmt_ppr=True, use_torch_sparse=True, use_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func(aggregate_type='mlp', alpha=0.15, data_strategy='fix-all', dev_per_lb=0.1, exp_name='chameleon-debug-1', graph_dataset_name='squirrel', graph_snapshot_basetime=150000.0, graph_snapshot_interval=15000.0, hidden_size_mlps=[32, 16], is_dangling_avoid=True, is_retrain_each_snapshot=True, local_proj_cache_dir='/home/xingzguo/git_project/DynMixer/cache', local_proj_data_dir='/home/xingzguo/projects_data/DynMixer', min_epoch_train=5, model_max_train_epochs=1000, n_cpu=4, num_mlps=2, optim_lr=0.05, ppe_out_dim=128, ppr_algo='ista', pprgo_topk=32, rs=621, test_per_lb=0.2, total_sampled_node=2000.0, train_per_lb=0.7, use_cuda=True, use_incrmt_ppr=False, use_torch_sparse=True, use_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import functorch\n",
    "\n",
    "t = torch.tensor([[1., 1], [2., 3.]])\n",
    "print(t)\n",
    "t_sp = t.to_sparse()\n",
    "torch.sparse.softmax(t_sp, dim=1)\n",
    "\n",
    "\n",
    "def poly_func(x, poly_coefs):\n",
    "    out = torch.zeros_like(x)\n",
    "    for k in range(poly_coefs.shape[0]):\n",
    "        out += (x**k) * (poly_coefs[k])\n",
    "    return out\n",
    "\n",
    "poly_func(\n",
    "    torch.tensor([2.0, 0.0]), \n",
    "    torch.tensor([1.0, 1.0, 1.0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "poly_coefs =  Variable(torch.randn(3).type(torch.float), requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[2.0, 0.0], [1.0, 1.0]])\n",
    "zero_mask = (x == 0.0)\n",
    "print(zero_mask)\n",
    "out = functorch.vmap(poly_func)(\n",
    "    x, \n",
    "    poly_coefs = poly_coefs,\n",
    "    )\n",
    "out[zero_mask] = 0.0\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_coefs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f969c412730>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def poly_func(x, poly_coefs):\n",
    "    out = np.zeros_like(x)\n",
    "    # skip offset\n",
    "    for k in range(poly_coefs.shape[0] - 1):\n",
    "        out += (x ** (k + 1)) * (poly_coefs[k + 1])\n",
    "    return out\n",
    "\n",
    "x = np.arange(0,0.4,0.01)\n",
    "a = [0]*5\n",
    "\n",
    "# a = [ 0.0000, -0.0330, -0.0279, -0.0081, -0.0021] # film\n",
    "# a = [0.0000, 0.0779, 0.0294, 0.0066, 0.0015] # chameleon\n",
    "# a = [0.0000, 0.2397, 0.2365, 0.2291, 0.2168] # cora\n",
    "# a = [ 0.0000, 30.9898, 26.9427, 21.4760, 17.4812]\n",
    "# a = [ 0.0000, 34.3352, 30.4187, 25.5207, 21.1020]\n",
    "a = [0., 1., 0., 0., 0.]\n",
    "poly_coefs = np.array(a)\n",
    "y = poly_func(x, poly_coefs)\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,3,0,0,0,1,2])\n",
    "b = sp.csr_array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 0, 0, 0, 1, 2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1542591750621796"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mat_to_bytes(nrows, ncols, dtype=32, out=\"GB\"):\n",
    "    \"\"\"Calculate the size of a numpy array in bytes.\n",
    "        :param nrows: the number of rows of the matrix.\n",
    "        :param ncols: the number of columns of the matrix.\n",
    "        :param dtype: the size of each element in the matrix. Defaults to 32bits.\n",
    "        :param out: the output unit. Defaults to gigabytes (GB)\n",
    "        :returns: the size of the matrix in the given unit\n",
    "        :rtype: a float\n",
    "        \"\"\"\n",
    "    sizes = {v: i for i, v in enumerate(\"BYTES KB MB GB TB\".split())}\n",
    "    return nrows * ncols * dtype / 8 / 1024. ** sizes[out]\n",
    "\n",
    "mat_to_bytes(5000, 169343, 32, \"GB\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "filepath = \"/home/xingzguo/projects_data/DynMixer/ogbn-products/raw_data/raw_node_label.pkl\"\n",
    "with open(filepath,'rb') as f:\n",
    "    lbs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2379138, 47)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "lb_arr = []\n",
    "t_lim = 60_000_000\n",
    "for (key, arr, t) in lbs:\n",
    "    if t > t_lim:\n",
    "        break\n",
    "    lb_arr.append(arr)\n",
    "lb_arr = np.array(lb_arr, dtype=bool)\n",
    "print (lb_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111581, 107886, 111829, 142459, 644135,  40146, 156729, 168462,\n",
       "       108965,  65798,  51221,  28783, 130017,  99269,   3023,  26713,\n",
       "        82134,  41695,  47918,  17095,  22227,  79935,    856,   3576,\n",
       "        43344,   2994,    542,    253,   1822,   1475,    233,    410,\n",
       "          506,     29,    153,     43,    610,    510,     91,     37,\n",
       "            2,     59,  31690,   1321,    552,      9,      1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_arr.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2379138"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lb_arr.sum(axis = 1) !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111581, 107886, 111829, 142459, 644135,  40146, 156729, 168462,\n",
       "       108965,  65798,  51221,  28783, 130017,  99269,   3023,  26713,\n",
       "        82134,  41695,  47918,  17095,  22227,  79935,    856,   3576,\n",
       "        43344,   2994,    542,    253,   1822,   1475,    233,    410,\n",
       "          506,     29,    153,     43,    610,    510,     91,     37,\n",
       "            2,     59,  31690,   1321,    552,      9,      1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_arr.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       nan 0.         0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2701/3254348246.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ], dtype=np.float\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def simulate_additive_noise(\n",
    "    original_feat_mat: np.ndarray,\n",
    "    snapshot_id,\n",
    "    total_snapshot,\n",
    "    base_original_sign: float = 0.4,\n",
    "):\n",
    "    # Simulate\n",
    "    # number of features\n",
    "    original_feat = np.copy(original_feat_mat)\n",
    "    num_row, num_col = original_feat.shape\n",
    "    origin_p = base_original_sign + (1 - base_original_sign) * (\n",
    "        snapshot_id\n",
    "    ) / (total_snapshot - 1)\n",
    "    noise_p = 1.0 - origin_p\n",
    "    mu = np.mean(original_feat, axis=1)\n",
    "    print (mu)\n",
    "    nnz_ids = ~np.isnan(mu)\n",
    "    # print(nnz_ids)\n",
    "    mu_nnz = np.mean(original_feat[nnz_ids])\n",
    "    std_nnz = np.std(original_feat[nnz_ids])\n",
    "    # print(mu_nnz, std_nnz )\n",
    "    noise_mat = np.random.normal(mu_nnz, std_nnz, (nnz_ids.sum(), num_col))\n",
    "    original_feat[nnz_ids] = origin_p*original_feat[nnz_ids] + noise_p*noise_mat\n",
    "\n",
    "    # print(snapshot_id, total_snapshot, origin_p, noise_p)\n",
    "    return original_feat\n",
    "\n",
    "original_feat_mat = np.array(\n",
    "    [\n",
    "        [np.nan,np.nan,np.nan],\n",
    "        [0,0,0],\n",
    "        [1,0,1],\n",
    "    ], dtype=np.float\n",
    ")\n",
    "\n",
    "simulate_additive_noise(original_feat_mat,3,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "import torch\n",
    "import os\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "\n",
    "\n",
    "class EllipticBitcoinTempDataset(InMemoryDataset):\n",
    "    r\"\"\"The Elliptic Bitcoin dataset of Bitcoin transactions from the\n",
    "    `\"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional\n",
    "    Networks for Financial Forensics\" <https://arxiv.org/abs/1908.02591>`_\n",
    "    paper.\n",
    "\n",
    "\n",
    "        * - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - 203,769\n",
    "          - 234,355\n",
    "          - 165\n",
    "          - 2\n",
    "    \"\"\"\n",
    "    url = 'https://data.pyg.org/datasets/elliptic'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'elliptic_txs_features.csv',\n",
    "            'elliptic_txs_edgelist.csv',\n",
    "            'elliptic_txs_classes.csv',\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for file_name in self.raw_file_names:\n",
    "            path = download_url(f'{self.url}/{file_name}.zip', self.raw_dir)\n",
    "            extract_zip(path, self.raw_dir)\n",
    "            os.remove(path)\n",
    "\n",
    "    def _process_df(self, feat_df: Any, edge_df: Any,\n",
    "                    class_df: Any) -> Tuple[Any, Any, Any]:\n",
    "        \n",
    "        print('test', feat_df )\n",
    "        \n",
    "        return feat_df, edge_df, class_df\n",
    "\n",
    "    def process(self):\n",
    "        import pandas as pd\n",
    "\n",
    "        feat_df = pd.read_csv(self.raw_paths[0], header=None)\n",
    "        edge_df = pd.read_csv(self.raw_paths[1])\n",
    "        class_df = pd.read_csv(self.raw_paths[2])\n",
    "\n",
    "        columns = {0: 'txId', 1: 'time_step'}\n",
    "        feat_df = feat_df.rename(columns=columns)\n",
    "\n",
    "        feat_df, edge_df, class_df = self._process_df(\n",
    "            feat_df,\n",
    "            edge_df,\n",
    "            class_df,\n",
    "        )\n",
    "\n",
    "        x = torch.from_numpy(feat_df.loc[:, 2:].values).to(torch.float)\n",
    "\n",
    "        # There exists 3 different classes in the dataset:\n",
    "        # 0=licit,  1=illicit, 2=unknown\n",
    "        mapping = {'unknown': 2, '1': 1, '2': 0}\n",
    "        class_df['class'] = class_df['class'].map(mapping)\n",
    "        y = torch.from_numpy(class_df['class'].values)\n",
    "\n",
    "        mapping = {idx: i for i, idx in enumerate(feat_df['txId'].values)}\n",
    "        edge_df['txId1'] = edge_df['txId1'].map(mapping)\n",
    "        edge_df['txId2'] = edge_df['txId2'].map(mapping)\n",
    "        # edge_df should sort\n",
    "        edge_index = torch.from_numpy(edge_df.values).t().contiguous()\n",
    "        \n",
    "\n",
    "        # Timestamp based split:\n",
    "        # train_mask: 1 - 34 time_step, test_mask: 35-49 time_step\n",
    "        time_step = torch.from_numpy(feat_df['time_step'].values)\n",
    "        train_mask = (time_step < 35) & (y != 2)\n",
    "        test_mask = (time_step >= 35) & (y != 2)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                    test_mask=test_mask)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature torch.Size([203769, 165])\n",
      "label tensor([2, 2, 2,  ..., 1, 2, 2])\n",
      "0 203768\n",
      "[[     0      2      4 ... 201921 201480 201954]\n",
      " [     1      3      5 ... 202042 201368 201756]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_pyg = EllipticBitcoinTempDataset(\n",
    "    root = './',\n",
    ")\n",
    "\n",
    "edge_list = dataset_pyg[0].edge_index.numpy().astype(np.int32)\n",
    "# print('edge-list', edge_list.shape)\n",
    "print('feature', dataset_pyg[0].x.shape)\n",
    "print('label', dataset_pyg[0].y)\n",
    "print(np.min(edge_list), np.max(edge_list)) # index from zero\n",
    "print(edge_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('DynGL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5d2ccb7684c46d8ee02d390ebe5b031224e4f061fd3cbf251699a3eb548b24b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
